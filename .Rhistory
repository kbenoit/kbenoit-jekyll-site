}
files <- list.files(".", ".md$", recursive = TRUE)
str <- NULL
for(fn in files){
fc <- readLines(fn)
print(length(fc))
}
library(stringr)
library(dplyr)
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\.kenbenoit\.net/files") %>% length()
}
library(dplyr)
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\\.kenbenoit\\.net/files") %>% length()
}
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\\.kenbenoit\\.net/files") %>% length() %>% print()
}
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\\.kenbenoit\\.net/files") %>% print()
}
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\\.kenbenoit\\.net/files")[,2] %>% print()
}
for(fn in files){
fc <- readLines(fn)
str_match(fc, "http://www\\.kenbenoit\\.net/files")[,2] #%>% print()
}
for(fn in files){
fc <- readLines(fn)
grep("http://www\\.kenbenoit\\.net/files", fc)
}
warnings
warnings()
for(fn in files){
fc <- readLines(fn)
grep("http://www\\.kenbenoit\\.net/files", fc) %>%
length() %>%
print()
}
str <- NULL
library(dplyr)
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/files", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
fc
library(stringr)
str <- NULL
library(dplyr)
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/files", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/files", fc)]
#if(length(fc) > 0){
str <- c(str, fc)
#}
}
fc
library(stringr)
str <- NULL
library(dplyr)
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/files", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
str
str_replace_all(str, ".+http://www\\.kenbenoit\\.net/files/(.+)\\).+")
str_replace_all(str, ".+http://www\\.kenbenoit\\.net/files/(.+)\\).+","\\1")
str_replace_all(str, ".+http://www\\.kenbenoit\\.net/files/(.+?)\\).+","\\1")
str_replace_all(str, ".+http://www\\.kenbenoit\\.net/files/(.+?)\\).*","\\1")
fn_cp <- str_replace_all(str, ".+http://www\\.kenbenoit\\.net/files/(.+?)\\).*","\\1")
for(fn in fn_cp) file.copy(paste0("~/Downloads/files/", fn),
paste0("assets/files/", fn))
str <- NULL
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/wp-contents", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
str
fc <- fc[grep("http://www\\.kenbenoit\\.net/wp-content", fc)]
str <- NULL
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/wp-content", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
str
str_match_all(str, ".+http://www\\.kenbenoit\\.net/wp-content/(.+?)\\).*")
str
str_match_all(str, ".+http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|").*")
str_match_all(str, ".+http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|\\").*")
str_match_all(str, ".+http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|a).*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|a).*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|\\").*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|\").*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/(.+?)(\)|\").*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/(.+?)(\\)|\").*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\").*")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")")
str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")") %>% rbind_list()
out <- str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")")
rbind_list(out)
bind_rows(out)
?bind_rows
bind_rows(unlist(out))
unlist(out)
out <- str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")") [, 1]
out <- str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")") %>%
lapply(, function(x) x[,1])
out <- str_match_all(str, "http://www\\.kenbenoit\\.net/wp-content/.+?(\\)|\")") %>%
lapply(function(x) x[,1])
out
out <- str_match_all(str, "(http://www\\.kenbenoit\\.net/wp-content/.+?)(\\)|\")") %>%
lapply(function(x) x[,2]) %>% unlist
out
out <- str_match_all(str, "(http://www\\.kenbenoit\\.net/wp-content/.+?)(\\)|\"| )") %>%
lapply(function(x) x[,2]) %>% unlist
out
download.file
?download.file
for(fn in out) download.file(fn, paste0("assets/images/", basename(fn)))
str <- NULL
for(fn in files){
fc <- readLines(fn)
fc <- fc[grep("http://www\\.kenbenoit\\.net/courses", fc)]
if(length(fc) > 0){
str <- c(str, fc)
}
}
str
out <- str_match_all(str, "(http://www\\.kenbenoit\\.net/courses/.+?)(\\)|\"| )") %>%
lapply(function(x) x[,2]) %>% unlist
out
?basename
out
for(fn in out) {
new_fn <- str_replace("http://www\\.kenbenoit\\.net/courses/", "courses/")
if(!dir.exists(dirname(new_fn))) dir.create(dirname(new_fn))
download.file(fn, new_fn)
}
for(fn in out) {
new_fn <- str_replace(fn, "http://www\\.kenbenoit\\.net/courses/", "courses/")
if(!dir.exists(dirname(new_fn))) dir.create(dirname(new_fn))
download.file(fn, new_fn)
}
fn
new_fn
dir.create("assets/courses")
for(fn in out) {
new_fn <- str_replace(fn, "http://www\\.kenbenoit\\.net/courses/", "assets/courses/")
if(!dir.exists(dirname(new_fn))) dir.create(dirname(new_fn))
download.file(fn, new_fn)
}
?download.file
?try
for(fn in out) {
new_fn <- str_replace(fn, "http://www\\.kenbenoit\\.net/courses/", "assets/courses/")
if(!dir.exists(dirname(new_fn))) dir.create(dirname(new_fn))
try(download.file(fn, new_fn))
}
?wordcloud::comparison.cloud
?quanteda::textplot_wordcloud
?topfeatures
library("quanteda")
?topfeatures
f <- data_corpus_irishbudget2010 %>% dfm(remove_punct = TRUE) %>% topfeatures()
head(f, 20)
zipflm <- lm(log(f[1:1000]) ~ log(1:1000))
plot(abline(zipflm))
plot(f[1:1000]) ~ log(1:1000))
plot(f[1:1000]) ~ log(1:1000)
plot(f[1:1000] ~ log(1:1000))
abline(zipflm)
plot(f[1:1000] ~ 1:1000, log = "xy")
plot(f[1:1000] ~ I(1:1000), log = "xy")
abline(zipflm)
plot(log(f[1:1000]) ~ I(log(1:1000)))
abline(zipflm)
f <- data_corpus_irishbudget2010 %>% dfm(remove_punct = TRUE) %>% topfeatures()
head(f, 100)
f
f <- data_corpus_irishbudget2010 %>% dfm(remove_punct = TRUE) %>% topfeatures(100)
f
install.packages("quanteda")
install.packages(c("BH", "callr", "digest", "haven", "hms", "htmlwidgets", "knitr", "lubridate", "microbenchmark", "openssl", "pbdZMQ", "Rcpp", "RcppEigen", "RcppParallel", "reprex", "reticulate", "tibble", "tidyr", "unrtf", "usethis", "viridis", "viridisLite", "vote", "xml2"))
install.packages("stmgui")
`/stmgui`
?stmgui
help(package = "stmgui")
runStmGui()
library(stmgui)
runStmGui()
install.packages("DT")
runStmGui()
dfm.object1 <- dfm(c("the quick brown fox jumps over the lazy dog", "the quick brown foxy ox jumps over the lazy god"))
dfm.object1
dfm.trimmed1 <- dfm_trim(dfm.object1, min_docfreq = 0.9)
dfm.trimmed1
dfm.tfidf1 <- dfm_tfidf(dfm.trimmed1)
dfm.tfidf1
dfm.object2 <- dfm(c("the quick brown fox jumps over the lazy dog", "the quick brown foxy ox jumps over the lazy god"))
dfm.object2
dfm.tfidf2 <- dfm_tfidf(dfm.object2)
dfm.tfidf2
dfm.trimmed2 <- dfm_trim(dfm.tfidf2, min_docfreq = 0.9)
data_dfm_lbgexample %>%
dfm_tfidf() %>%
dfm_trim()
data_char_sampletext
myCorpus <- corpus(data_char_sampletext) %>%
corpus_reshape(to = "sentences")
myCorpus
library("quanteda")
myCorpus <- corpus(data_char_sampletext) %>%
corpus_reshape(to = "sentences")
myCorpus
docvars(myCorpus, "author") <-
sample(c("author1", "author2", "author3"), replace = TRUE)
summary(myCorpus)
set.seed(10)
docvars(myCorpus, "author") <-
sample(c("author1", "author2", "author3"), replace = TRUE)
summary(myCorpus)
ndoc(groupedtexts)
groupedtexts <- texts(myCorpus, groups = "author")
ndoc(groupedtexts)
length(groupedtexts)
?sample
docvars(myCorpus, "author") <-
sample(c("author1", "author2", "author3"),
size = ndoc(myCorpus), replace = TRUE)
summary(myCorpus)
set.seed(1)
docvars(myCorpus, "author") <-
sample(c("author1", "author2", "author3"),
size = ndoc(myCorpus), replace = TRUE)
summary(myCorpus)
groupedtexts <- texts(myCorpus, groups = "author")
length(groupedtexts)
str(groupedtexts)
textplot_xray(
kwic(groupedtexts, "like"),
kwic(groupedtexts, "time")
)
textplot_xray(
kwic(groupedtexts, "like"),
kwic(groupedtexts, "time")
)
groupedtexts <- texts(myCorpus, groups = "author")
textplot_xray(
kwic(groupedtexts, "like"),
kwic(groupedtexts, "time")
)
textplot_xray(
kwic(groupedtexts, "and"),
kwic(groupedtexts, "for")
)
names(groupedtexts)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "##"
)
m <- matrix(c(0, 1, 3, 0, 1, 0, 5, 0, 2, 0, 6, 4), nrow = 3,
dimnames = list(docs = paste0("doc", 1:3),
features = LETTERS[1:4]))
m
?read_excel
??read_excel
germancorp <- c("a b c", "d d e f f")
dfm(germancorp, ngrams = c(2, 3),
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("german"),
stem = TRUE)
library("quanteda")
library("readtext")
dfm(germancorp, ngrams = c(2, 3),
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("german"),
stem = TRUE)
germancorp <- c("ate eating eated", "d d e f f")
dfm(germancorp, ngrams = c(2, 3),
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("german"),
stem = TRUE)
dtm <- dfm_trim(dtm, min_count = 2)
# form tf-idf weights
dtm <- dfm_tfidf(dtm)
# if you really want a tm formatted DocumentTermMatrix
convert(dtm, to = "tm")
dtm <- dfm(germancorp, ngrams = c(2, 3),
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove = stopwords("german"),
stem = TRUE)
# remove words with only a single count
dtm <- dfm_trim(dtm, min_count = 2)
# form tf-idf weights
dtm <- dfm_tfidf(dtm)
# if you really want a tm formatted DocumentTermMatrix
convert(dtm, to = "tm")
?wordcloud
summary(data_corpus_irishbudget2010)
summary(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(data_corpus_irishbudget2010, remove_punct = TRUE, remove_numbers = TRUE)
kwic(data_corpus_irishbudget2010, pattern = "christmas")
kwic(data_corpus_irishbudget2010, pattern = "christmas*")
?kwic
kwic(data_corpus_irishbudget2010, pattern = "disaster")
kwic(data_corpus_irishbudget2010, pattern = "disaster", valuetype = "glob")
kwic(data_corpus_irishbudget2010, pattern = "disaster", valuetype = "fixed")
kwic(data_corpus_irishbudget2010, pattern = "disast*", valuetype = "fixed")
kwic(data_corpus_irishbudget2010, pattern = "disast*", valuetype = "glob")
kwic(data_corpus_irishbudget2010, pattern = "disast*", valuetype = "glob", window = 3)
kwic(data_corpus_irishbudget2010, pattern = "disast*", valuetype = "glob", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "^disast", valuetype = "glob", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "^disast", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "disast", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "ast", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "colou*r", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "neigbou*r", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "honou*r", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "honou{0,1}r", valuetype = "regex", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "hono*r", valuetype = "glob", window = 2)
kwic(data_corpus_irishbudget2010, pattern = "hono*r*", valuetype = "glob", window = 2)
tokens("The 10 foxes jumped over, the dog!")
tokens("The 10 foxes jumped over, the dog!", remove_sep = FALSE)
txt <- "The factory will produce ten cars next year.  I shop for cabbage in the produce section."
dfm(txt)
sp <- spacyr::spacy_parse(txt)
sp
sptoks <- as.tokens(sp)
sptoks <- as.tokens(sp, include_pos = "pos")
sptoks
dfm(sptoks)
dfm(sptoks, tolower = FALSE)
tokens_select(sptoks, "*/NOUN")
tokens_remove(sptoks, "*/DET")
sptoks
spacyr::spacy_parse(txt, pos = "tag")
?spacy_parse
spacyr::spacy_parse(txt, pos = TRUE, tag = TRUE)
coll <- textstat_collocations(tokens(data_corpus_inaugural[58]), method = "lambda")
head(coll)
tokens(data_corpus_inaugural[58]) %>% tokens_remove(stopwords()) %>% textstat_collocations()
tokens(data_corpus_inaugural[58]) %>% tokens_remove(stopwords(), padding = TRUE) %>% textstat_collocations()
tokens(data_corpus_inaugural[58]) %>% tokens_remove(stopwords(), padding = TRUE) %>% textstat_collocations(size = 4)
tokens(data_corpus_inaugural[58]) %>% tokens_remove(stopwords(), padding = TRUE) %>% textstat_collocations(size = 4) %>% View
tokens(data_corpus_inaugural[58]) %>% tokens_remove(stopwords(), padding = TRUE) %>% textstat_collocations(size = 2) %>% View
tokens(data_corpus_inaugural[58]) %>% kwic(phrase("make america"))
tokens(data_corpus_inaugural[58]) %>% kwic("make america")
tokens(data_corpus_inaugural[58]) %>% kwic(list(c("make", "america"))
)
spacy_parse(data_corpus_inaugural[58]) %>% spacyr::entity_consolidate()
id <- dfm(data_corpus_inaugural)
itoks <- tokens(data_corpus_inaugural)
tokens_remove(itoks, verbose = TRUE)
itoks2 <- tokens_remove(itoks, verbose = TRUE)
itoks2 <- tokens_remove(itoks, stopwords("english"), verbose = TRUE)
length(stopwords("english"))
itoks2 <- tokens_tolower() %>% tokens_remove(itoks, stopwords("english"), verbose = TRUE)
itoks2 <- tokens_tolower(itoks) %>% tokens_remove(stopwords("english"), verbose = TRUE)
itoks2 <- tokens_tolower(itoks) %>% tokens_remove(stopwords("english"), verbose = TRUE) %>% tokens_remove(pattern = "\\p{P}", valuetype = "regex", verbose = TRUE)
length(types(itoks))
length(types(tokens(data_corpus_inaugural, remove_punct = TRUE)))
length(types(tokens(data_corpus_inaugural, remove_punct = FALSE)))
itoks2 <- tokens_tolower(itoks) %>% tokens_remove(stopwords("english"), verbose = TRUE) %>% tokens_remove(pattern = "^\\p{P}$", valuetype = "regex", verbose = TRUE)
?tokens_replace
dict <- read.csv("~/Downloads/data_dict_usbr.csv", stringsAsFactors = FALSE)
dict
dict <- rbind(dict, c("tokenise", "tokenize"))
dict
tail(dict)
# sort
dict <- dict[order(dict$british), ]
d <- dict[1:3, ]
d
as.list(d)
list(apply(d, 1, function(x) list(x[1] = x[2]))
list(apply(d, 1, function(x) l <- list(x[2]); names(l) <- x[1]; x)
list(apply(d, 1, function(x) { l <- list(x[2]); names(l) <- x[1]; x })
)
lapply(d, function(x) { l <- list(x[2]); names(l) <- x[1]; x }))
lapply(d, function(x) { l <- list(x[2]); names(l) <- x[1]; x })
apply(d, 1, function(x) { l <- list(x[2]); names(l) <- x[1]; x })
d
l <- list(dict$american)
names(l) <- list(dict$british)
data_dictionary_uk2us <- dictionary(l)
data_dictionary_uk2us
d
as.list(d$american)
# data_dictionary_uk2us
l <- stringi::stri_trim_both(as.list(dict$american))
names(l) <- list(dict$british)
l
# data_dictionary_uk2us
dict <- d
# data_dictionary_uk2us
l <- stringi::stri_trim_both(as.list(dict$american))
names(l) <- list(dict$british)
dict <- read.csv("~/Downloads/data_dict_usbr.csv", stringsAsFactors = FALSE)
# add tokenize
dict <- rbind(dict, c("tokenise", "tokenize"))
# sort
dict <- dict[order(dict$british), ]
dict <- dict[1:3, ]
dict <- read.csv("~/Downloads/data_dict_usbr.csv", stringsAsFactors = FALSE)
# add tokenize
dict <- rbind(dict, c("tokenise", "tokenize"))
# sort
dict <- dict[order(dict$british), ]
dict <- dict[1:3, ]
stringi::stri_trim_both(dict$american)
# data_dictionary_uk2us
l <- as.list(stringi::stri_trim_both(dict$american))
l
names(l) <- list(dict$british)
l
dict$british
dict
stringi::stri_trim_both(dict)
lapply(dict, stringi::stri_trim_both)
dict <- as.data.frame(lapply(dict, stringi::stri_trim_both))
str(dict)
dict <- as.data.frame(lapply(dict, stringi::stri_trim_both), stringsAsFactors = FALSE)
dict
str(dict)
# add tokenize
dict <- rbind(dict, c("tokenise", "tokenize"))
# sort
dict <- dict[order(dict$british), ]
dict
# data_dictionary_uk2us
l <- as.list(stringi::stri_trim_both(dict$american))
names(l) <- list(dict$british)
l
names(l) <- dict$british
l
?as.list
dict <- read.csv("~/Downloads/data_dict_usbr.csv", stringsAsFactors = FALSE)
# trim the whitespace pads
dict <- as.data.frame(lapply(dict, stringi::stri_trim_both), stringsAsFactors = FALSE)
# add tokenize
dict <- rbind(dict, c("tokenise", "tokenize"))
# sort
dict <- dict[order(dict$british), ]
# data_dictionary_uk2us
l <- as.list(dict$american)
names(l) <- dict$british
data_dictionary_uk2us <- dictionary(l)
# data_dictionary_us2uk
l <- as.list(dict$british)
names(l) <- dict$american
data_dictionary_us2uk <- dictionary(l)
data_dictionary_us2uk
# data_dictionary_us2uk
l <- as.list(dict$american)
names(l) <- dict$british
data_dictionary_us2uk <- dictionary(l)
# data_dictionary_uk2us
l <- as.list(dict$british)
names(l) <- dict$american
data_dictionary_uk2us <- dictionary(l)
log(2.5)
log10(2.5)
log10(2.5) * 16
log10(100/40) * 16
log10(100/15) * 16
?quanteda
?news
news()
news(package = "quanetda")
news(package = "quanteda")
?news
tokens(data_char_ukimmig2010) %>% tokens_wordstem(verbose = TRUE)
tokens(data_char_ukimmig2010) %>% tokens_wordstem()
?tokens_wordstem
tokens(data_char_ukimmig2010) %>% ntoken()
tokens(data_char_ukimmig2010) %>% tokens_wordstem() %>% ntoken
tokens(data_char_ukimmig2010) %>% tokens_wordstem() %>% ntype()
tokens(data_char_ukimmig2010) %>% ntype()
